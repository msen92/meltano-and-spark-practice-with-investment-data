{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67395fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SPARK_HOME environment variable\n",
    "os.environ['SPARK_HOME']='/Users/mehmet.senturk/opt/miniconda3/envs/investment_insights/lib/python3.7/site-packages/pyspark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f088d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0deb0",
   "metadata": {},
   "source": [
    "# Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20864d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24163bf6",
   "metadata": {},
   "source": [
    "# Untyped Dataset Operations (aka DataFrame Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdbb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ea923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db771094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9b86",
   "metadata": {},
   "source": [
    "# Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a666b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a99df",
   "metadata": {},
   "source": [
    "# Global Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8bac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701c2ed",
   "metadata": {},
   "source": [
    "# Inferring the Schema Using Reflection\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc915929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fdc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = lines.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6db66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "# Name: Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff9259",
   "metadata": {},
   "source": [
    "# Programmatically Specifying the Schema\n",
    "\n",
    "When a dictionary of kwargs cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.\n",
    "\n",
    "1) Create an RDD of tuples or lists from the original RDD;\n",
    "2) Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.\n",
    "3) Apply the schema to the RDD via createDataFrame method provided by SparkSession.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8120111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa36945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2106ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9653ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c28551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fe699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9215bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb1639",
   "metadata": {},
   "source": [
    "# Generic Load/Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3144b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc791919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json to Parquet\n",
    "df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Csv\n",
    "df = spark.read.load(\"examples/src/main/resources/people.csv\",\n",
    "                     format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORC with bloom filter and dictonary encoding\n",
    "df = spark.read.orc(\"examples/src/main/resources/users.orc\")\n",
    "(df.write.format(\"orc\")\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\n",
    "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\n",
    "    .option(\"orc.column.encoding.direct\", \"name\")\n",
    "    .save(\"users_with_options.orc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46820de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet with bloom filter and dictonary encoding\n",
    "df = spark.read.parquet(\"examples/src/main/resources/users.parquet\")\n",
    "(df.write.format(\"parquet\")\n",
    "    .option(\"parquet.bloom.filter.enabled#favorite_color\", \"true\")\n",
    "    .option(\"parquet.bloom.filter.expected.ndv#favorite_color\", \"1000000\")\n",
    "    .option(\"parquet.enable.dictionary\", \"true\")\n",
    "    .option(\"parquet.page.write-checksum.enabled\", \"false\")\n",
    "    .save(\"users_with_options.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d2366",
   "metadata": {},
   "source": [
    "## Run SQL on files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04368af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out google sheets extraction\n",
    "df = spark.read.parquet('../output/Investments-*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ea6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"investments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c380098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted = spark.sql(\"\"\"\n",
    "select \n",
    "    investment_instrument\n",
    "    ,cast(investment_date as date) as investment_date\n",
    "    ,cast(amount as float) as amount \n",
    "    ,cast(price as float) as price\n",
    "from investments\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b14aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_insights",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4 (default, Aug 13 2019, 15:17:50) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d43b235a6cf488038c1b804f6005f5fcc1465b4b5fadf4aa7974dc62e30edc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
